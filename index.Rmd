---
title: "Inception Neural Networks for Complete Intersection Calabi-Yau Manifolds"
author: "<b>Riccardo Finotello (UniTO)</b> | based on [2007.13379](https://arxiv.org/abs/2007.13379) and [2007.15706](https://arxiv.org/abs/2007.15706) with H. Erbin (MIT & CEA-LIST)"
date: "<b>XVI Avogadro Meeting</b> - December 22nd, 2020"
output:
  ioslides_presentation:
    widescreen: yes
    smaller: yes
    transition: faster
    css: css/presentation.css
logo: img/unito.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>
  $(document).ready(function() {
    $('slide:not(.title-slide, .backdrop, .segue)').append('<footer label=\"Riccardo Finotello - Inception NN for CICY - 22/12/2020 - XVI Avogadro Meeting\"></footer>');
    })
</script>


## String Theory Compactification {.build}

<div class="centered">
**Superstrings** defined in $D = 10 \quad \Rightarrow \quad \mathcal{M}^{1, 9} = \mathcal{M}^{1, 3} \otimes X_6$
</div>

<div class="columns">
<div class="column-2">
<h3>Requirements</h3>
<ul>
<li>$X_6$ is a compact manifold $(M, g)$</li>
<li>$N = 1$ **SUSY** in 4D</li>
<li>**SM** $\subset$ arising gauge algebra</li>
</ul>
</div>

<div class="column-2">
<h3>Solution</h3>
<ul>
<li>$\dim_{\mathbb{C}} M = 3$</li>
<li>$\mathrm{Hol}(g) \subseteq \mathrm{SU}(3)$</li>
<li>$\mathrm{Ric}(g) \equiv 0$ or $c_1(M) \equiv 0$</li>
</ul>
</div>
</div>

<div class="centered">
<h3 class="fbox red2">**Calabi-Yau Manifolds**</h3>

<div class="column-1">
<ul>
<li>no known **metric** for compact CY</li>
<li>need to study **topology** (*Hodge numbers*) to infer **4D properties**</li>
</ul>
</div>

<div class="red2">
$$h^{r,s} = \dim_{\mathbb{C}} \mathrm{H}_{\overline{\partial}}^{r,s}(M, \mathbb{C})$$
</div>
</div>


## Complete Intersection Calabi-Yau Manifolds {.build}

<div>
Systems of **$k$ homogeneous equations** from products of **$m$ projective spaces**
</div>

<div class="centered red2">
$$
\sum\limits_{r = 1}^m p_{\alpha}^{i_r}\, \left( z_{i_r} \right)^{a^r_{\alpha}} = 0
\quad
\rightarrow
\quad
X =
\begin{bmatrix}
  \mathbb{P}^{n_1} &  |  & a^1_1  & \cdots & a^1_k
  \\
  \vdots           &     & \vdots & \ddots & \vdots
  \\
  \mathbb{P}^{n_m} &  |  & a^m_1  & \cdots & a^m_k
\end{bmatrix}
$$
</div>

<div>
such that
</div>

<div class="centered">
$$
\begin{matrix}
\text{degree of eq.} & \text{dim. of CY} & c_1 \equiv 0
\\
\Downarrow & \Downarrow & \Downarrow
\\
a^r_{\alpha} \in \mathbb{N}
&
\dim_{\mathbb{C}} X = \sum\limits_{r = 1}^m n_r - k = 3
&
n_r + 1 = \sum\limits_{\alpha = 1}^k a^r_{\alpha}
\end{matrix}
$$
</div>

<div>
where $a^r_{\alpha}$ are **powers** of coordinates on $\mathbb{P}^{n_r}$ in equation $\alpha$.
</div>


## Available Data

<div>
$\exists$ compiled datasets of **7890** CICY 3-folds with all **Hodge numbers** <span class="cite">[Green et al. (1987)]</span>
</div>

<div class="plot">
```{r data, cache=TRUE, out.width="100%"}
# location of the file
file.url <- "http://www.lpthe.jussieu.fr/~erbin/files/data/cicy3o_data.tar.gz"
file.dir <- file.path(".", "data")
file.out <- file.path(file.dir, "cicy3o_data.tar.gz")

# download the file
dir.create(file.dir, showWarnings = FALSE)
if (!file.exists(file.out)) {download.file(file.url, file.out)}

# extract the file
library(rhdf5)
data.name <- "cicy3o.h5"
data.loc  <- file.path(file.dir, data.name)
data.lab  <- h5read(data.loc, "/orig_cydata/block1_values")
data.mat  <- h5read(data.loc, "/orig_cydata/block4_values")

# create table
library(data.table)
DT <- data.table(h11=data.lab[2,], h21=data.lab[3,])

# produce plots
library(plotly)
h11 <- plot_ly(DT,
               x=~h11,
               type="histogram",
               alpha=0.35,
               name="h<sup>1,1</sup>",
               marker=list(line=list(color="darkgray", width=2)),
               hovertemplate="h<sup>1,1</sup> = %{x:d}<br>count = %{y:d}",
               showlegend = FALSE
               )

h21 <- plot_ly(DT,
               x=~h21,
               type="histogram",
               alpha=0.35,
               name="h<sup>2,1</sup>",
               marker=list(line=list(color="darkgray", width=2)),
               hovertemplate="h<sup>2,1</sup> = %{x:d}<br>count = %{y:d}",
               showlegend = FALSE
               )

hodge <- subplot(layout(h11,
                        yaxis=list(type="log", ticks="outside", tickvals=c(1, 10, 100, 1000)),
                        xaxis=list(title="h<sup>1,1</sup>", tickvals=c(0, 5, 10, 15, 20)),
                        plot_bgcolor="rgba(0, 0, 0, 0)",
                        paper_bgcolor="rgba(0, 0, 0, 0)"
                        ),
                 layout(h21,
                        yaxis=list(type="log", ticks="outside", tickvals=c(1, 10, 100, 1000)),
                        xaxis=list(title="h<sup>2,1</sup>", tickvals=c(0, 25, 50, 75, 100)),
                        plot_bgcolor="rgba(0, 0, 0, 0)",
                        paper_bgcolor="rgba(0, 0, 0, 0)"
                        ),
                 shareY=TRUE,
                 titleX=TRUE,
                 heights=0.75,
                 widths=c(0.5, 0.5)
                ) %>% config(displayModeBar=FALSE)
hodge
```
</div>


## Supervision and Function Approximation {.build}

<div class="centered">
$$
\begin{matrix}
\mathcal{R}\colon & \mathbb{N}^{m \times k} & \longrightarrow & \mathbb{N}^2
\\[0.5em]
                  & X                       & \mapsto         & (h^{1,1},\, h^{2,1})
\\[0.5em]
                  & \text{(conf. matrix)}   &                 & \text{(Hodge no.)}
\end{matrix}
$$
</div>

<div class="columns">
<div class="column-2">
<div class="fbox centered">
<h3 class="uppercase">Supervised Learning</h3>
</div>
<ul>
<li>
replace $\mathcal{R}( X )$ with $\mathcal{R}( X;\, W )$ ($W$ **weights**)
</li>
<li>
feed the algorithms $X$ and $h^{p,q}$ (**true values**)
</li>
<li>
<span class="centered red2">
$$
W = \arg\min\limits_{w}\; \mathcal{L}(h^{p,q},\, \mathcal{R}( X;\, w ))
$$
</span>
</li>
<li>
follow **gradient descent** of $\mathcal{L}$ $\rightarrow$ tune $W$
</li>
</ul>
</div>

<div class="column-2">
<div class="centered">
[![](https://imgs.xkcd.com/comics/machine_learning.png){width=55%}](https://xkcd.com/1838/)
</div>
</div>
</div>


## Neural Networks as Function Approximators {.build}

<div class="fbox centered">
<h3 class="uppercase">Fully Connected Networks</h3>
</div>

<div class="columns">
<div class="column-2">
![](img/fc.png){width=75%}
</div>

<div class="column-2">
<ul>
<li>older in design <span class="cite">[Rosenblatt (1958)]</span></li>
<li>analogy *neuron - "perceptron"*</li>
<li>rose to fame **in the 70s/80s**</li>
<li>
simple **matrix multiplications** $+$ **non linearity**
<span class="centered red2">
$$
a^{\{l+1\}} = \phi\left( w^{\{l\}} a^{\{l\}} + b^{\{l\}} \mathbb{I} \right) \in \mathbb{R}^p
$$
</span>
<span class="centered red2">
$$
\phi(z) = \mathrm{ReLU}(z) = \max(0, z)
$$
</span>
</li>
</div>
</div>

---

<div class="fbox centered">
<h3 class="uppercase">Convolutional Neural Networks</h3>
</div>

<div class="centered">
![](img/ccnn.png){width=80%}
</div>

<div class="columns">
<div class="column-2">
<ul>
<li>newer in conception <span class="cite">[LeCun et al. (1989)]</span></li>
<li>based on **sliding windows** (aka *convolutions*)
<span class="red2">
$$
a^{\{l+1\}} = \phi\left( w^{\{l\}} * a^{\{l\}} + b^{\{l\}} \mathbb{I} \right) \in \mathbb{R}^{p \times q}
$$
</span>
</li>
<li>**less parameters** to *isolate features*</li>
</ul>
</div>

<div class="column-2 centered">
![](img/conv.gif){width=60%}
</div>
</div>


## Inception Neural Networks

<div class="centered">
![](img/icnn.png){width=65%}
</div>

<div class="columns">
<div class="column-2">
<ul>
<li>inspired by **Google** <span class="cite">[Szegedy et al. (2014)]</span></li>
<li>created for **computer vision**</li>
<li>$2 \times 10^5$ parameters (vs $\ge 2 \times 10^6$)</li>
</ul>
</div>

<div class="column-2">
<ul>
<li>**concurrent kernels** $\Rightarrow$ shared parameters</li>
<li>retains **"spatial awareness"**</li>
<li>improved **generalisation ability**</li>
</ul>
</div>
</div>


## Results

<div class="plot">
```{r results, cache=TRUE, out.width="100%"}
# create data
library(data.table)
RES <- data.table(accuracy=c(0.77, 0.68, 0.95, 0.83, 0.99, 0.97, 0.36, 0.23, 0.50, 0.33),
                  hodge=factor(c("h<sup>1,1</sup>", "h<sup>1,1</sup>", "h<sup>1,1</sup>", "h<sup>1,1</sup>", "h<sup>1,1</sup>", "h<sup>1,1</sup>", "h<sup>2,1</sup>", "h<sup>2,1</sup>", "h<sup>2,1</sup>", "h<sup>2,1</sup>")),
                  type=factor(c("FC (previous best)", "FC (previous best)", "ConvNet", "ConvNet", "Inception", "Inception", "ConvNet", "ConvNet", "Inception", "Inception")),
                  ratio=factor(c("80% ratio", "30% ratio", "80% ratio", "30% ratio", "80% ratio", "30% ratio", "80% ratio", "30% ratio", "80% ratio", "30% ratio"))
                 )

p <- plot_ly(RES,
             x=~type,
             y=~accuracy,
             type="bar",
             color=~hodge,
             colors="Set1",
             frame=~ratio,
             alpha=0.5,
             text=paste(100 * RES$accuracy, "%"),
             textposition="outside",
             cliponaxis=FALSE,
             hovertemplate="%{x}: accuracy = %{text}",
             textfont=list(color="black"),
             showlegend=TRUE
            ) %>%
     animation_opts(500, easing="linear", redraw=FALSE) %>%
     animation_button(hide=TRUE) %>%
     animation_slider(currentvalue=list(prefix="")) %>%
     layout(title="Results of Different Architectures",
            xaxis=list(title="",
                       categoryorder="array",
                       categoryarray=RES$type
                      ),
            plot_bgcolor="rgba(0, 0, 0, 0)",
            paper_bgcolor="rgba(0, 0, 0, 0)"
            ) %>%
     config(displayModeBar=FALSE)
p
```
</div>

<div class="fright cite">[Erbin, RF (2020)]</div>


## Comparison

<div class="plot">
```{r comparison, cache=TRUE, out.width="100%"}
# create data
CFR <- data.table(implementations=factor(c("lin. reg. (80%)",
                                           "SVM RBF (reg., 30%)",
                                           "SVM RBF (reg., 80%)",
                                           "He [2017] (reg., 63%)",
                                           "Bull et al. [2018] (reg., 70%)",
                                           "Bull et al. [2018] (class, 70%)",
                                           "Inception (reg., 30%)",
                                           "Inception (reg., 80%)"
                                          )
                                         ),
                  accuracy=c(0.45, 0.55, 0.68, 0.37, 0.75, 0.85, 0.97, 0.99)
                 )

q <- plot_ly(CFR,
             x=~implementations,
             y=~accuracy,
             type="bar",
             colors="Set1",
             alpha=0.5,
             text=paste(100 * CFR$accuracy, "%"),
             textposition="outside",
             cliponaxis=FALSE,
             hovertemplate="%{x}: accuracy = %{text}",
             textfont=list(color="black"),
             name="h<sup>1,1</sup>",
             showlegend=TRUE
            ) %>%
     layout(title="Comparison of Accuracy for h<sup>1,1</sup>",
            xaxis=list(title="",
                       categoryorder="array",
                       categoryarray=CFR$implementations
                      ),
            margin=unit(c(0,0,0,0), "cm"),
            plot_bgcolor="rgba(0, 0, 0, 0)",
            paper_bgcolor="rgba(0, 0, 0, 0)"
            ) %>%
     config(displayModeBar=FALSE)
q
```
</div>

<div class="fright cite">[Erbin, RF (2020)]</div>


## Conclusion {.build}

<div class="centered fbox quote">
<p>*[Machine learning is]*</p>
<p>**the field of study that gives computers the ability to learn without being explicitly programmed.**</p>
<p class="signature">A. Samuel (1959)</p>
</div>

<div>
<ul>
<li>**deep learning** can be a **reliable predictive method**</li>
<li>it can be used as **source of inspiration** for **inference and generalisation**</li>
<li>**CNN**s have a lot of **unexpressed potential** in physics (*first time?*)</li>
<li>the approach intersects **mathematics**, **physics** and **computer science**</li>
</ul>

<h3>What Lies Ahead?</h3>

<ul>
<li>improve $h^{2,1}$ and **"un-blackbox" the model** (SHAP, filter analysis, etc.)</li>
<li>exploration for CICY 4-folds and **representation learning** (DGAN, VAE, RL, etc.)</li>
<li>study **symmetries** (GNNs, transformers architectures, etc.) and explore the **string landscape**</li>
</ul>
</div>

<div class="thankyou">
Thank You
</div>